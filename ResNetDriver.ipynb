{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet for CIFAR-10 \n",
    "**based on Section 4.2 in \"Deep Residual Learning for Image Recognition\" from Kaiming He, Xiangyu Zhang, Shaoqing Ren and Jian Sun**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# use GPU if available\n",
    "USE_GPU = True\n",
    "dtype = torch.float32 # we will be using float throughout this tutorial\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else: \n",
    "    device = torch.device('cpu')\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (initial_layer): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layerBlock1): ResidualBlocks(\n",
      "    (blocks): Sequential(\n",
      "      (0): ConvBlock(\n",
      "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (layerBlock2): ResidualBlocks(\n",
      "    (blocks): Sequential(\n",
      "      (0): ConvBlock(\n",
      "        (max): MaxPool2d(kernel_size=1, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (layerBlock3): ResidualBlocks(\n",
      "    (blocks): Sequential(\n",
      "      (0): ConvBlock(\n",
      "        (max): MaxPool2d(kernel_size=1, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
      "  (fc): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n",
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "# Check basic functionality of architecture\n",
    "from ResNet import ResNet, ResidualBlocks, ConvBlock\n",
    "\n",
    "def test_ResNet():\n",
    "    n = 1\n",
    "    x = torch.zeros((64, 3, 32, 32), dtype=dtype) # minibatch size 64\n",
    "    model = ResNet(n)\n",
    "    print(model)\n",
    "    scores = model(x)\n",
    "    print(scores.size())\n",
    "    \n",
    "test_ResNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# import CIFAR data\n",
    "\n",
    "NUM_TRAIN = 45000\n",
    "batch_size = 128\n",
    "\n",
    "# The torchvision.transforms package provides tools for preprocessing data\n",
    "# and for performing data augmentation; here we set up a transform to process\n",
    "# the data by subtracting the means RGB value and dividing by the \n",
    "# standard deviation of each RGB value; we've hardcoded the mean and std.\n",
    "transform = T.Compose([\n",
    "                T.ToTensor(),\n",
    "                T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "            ])\n",
    "\n",
    "# We set up a Dataset object for each split (train / val / test); Datasets load\n",
    "# training examples one at a time, so we wrap each Dataset in a DataLoader which\n",
    "# iterates through the Dataset and forms minibatches. We divide the CIFAR-10\n",
    "# training set into train and val sets by passing a Sampler object to the\n",
    "# DataLoader telling how it should sample from the underlying Dataset.\n",
    "\n",
    "cifar10_train = dset.CIFAR10('./cs231n/datasets', train=True, download=True, transform=transform)\n",
    "loader_train = DataLoader(cifar10_train, batch_size=batch_size, sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n",
    "\n",
    "cifar10_val = dset.CIFAR10('./cs231n/datasets', train=True, download=True, transform=transform)\n",
    "loader_val = DataLoader(cifar10_val, batch_size=batch_size, sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN, 50000)))\n",
    "\n",
    "cifar10_test = dset.CIFAR10('./cs231n/datasets', train=False, download=True, transform=transform)\n",
    "loader_test = DataLoader(cifar10_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(loader, model):\n",
    "    if loader.dataset.train:\n",
    "        print('Checking accuracy on validation set')\n",
    "    else: \n",
    "        print('Checking accuracy on test set')\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval() # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=dtype) # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (y == preds).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
    "        \n",
    "        \n",
    "def check_accuracy_batch(scores, labels, verbose=False):\n",
    "    _, preds = scores.max(1)\n",
    "    num_correct = (labels == preds).sum()\n",
    "    num_samples = preds.size(0)\n",
    "    acc = float(num_correct) / num_samples\n",
    "    if verbose:\n",
    "        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and validation function for one epoch\n",
    "\n",
    "def train_ResNet(model, optimizer, dataloader):\n",
    "    \"\"\"\n",
    "    Train a model on CIFAR-10 using the PyTorch Module API.\n",
    "    \n",
    "    Inputs:\n",
    "    - model: (torch.nn.Module) A PyTorch module giving the model to train.\n",
    "    - optimizer: (torch.optim) An optimizer object we will use to train the model\n",
    "    - dataloader: (DataLoader) A DataLoader object supplying the training data\n",
    "    \n",
    "    Returns: Nothing, but prints model accuracies during training.\n",
    "    \"\"\"\n",
    "    # set model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # loss history \n",
    "    loss_history = []\n",
    "    train_accuracy_history = []\n",
    "    \n",
    "    for it, (train_batch, labels_batch) in enumerate(dataloader):\n",
    "        verbose = False\n",
    "        if it % 10 == 0:\n",
    "            verbose = True\n",
    "        \n",
    "        # compute scores and loss\n",
    "        scores = model(train_batch)\n",
    "        loss = F.cross_entropy(scores, labels_batch)\n",
    "        \n",
    "        accuracy = check_accuracy_batch(scores, labels_batch, verbose=verbose)\n",
    "        loss_history.append(loss)\n",
    "        train_accuracy_history.append(accuracy)\n",
    "    \n",
    "        # clear previous gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # build gradient computational graph\n",
    "        loss.backward()\n",
    "        \n",
    "        # actual backprop und updating params\n",
    "        optimizer.step()\n",
    "        \n",
    "    return loss_history, train_accuracy_history\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def evaluate_ResNet(model, dataloader):\n",
    "    \"\"\"\n",
    "    Train a model on CIFAR-10 using the PyTorch Module API.\n",
    "    \n",
    "    Inputs:\n",
    "    - model: (torch.nn.Module) A PyTorch module giving the model to train.\n",
    "    - optimizer: (torch.optim) An optimizer object we will use to train the model\n",
    "    - dataloader: (DataLoader) A DataLoader object supplying the training data\n",
    "    \n",
    "    Returns: Nothing, but prints model accuracies during training.\n",
    "    \"\"\"\n",
    "    # set model to test mode\n",
    "    model.eval()\n",
    "    \n",
    "    # loss history \n",
    "    loss_history = []\n",
    "    test_accuracy_history = []\n",
    "    \n",
    "    for it, (test_batch, labels_batch) in enumerate(dataloader):\n",
    "  \n",
    "        # compute scores and loss\n",
    "        scores = model(test_batch)\n",
    "        loss = F.cross_entropy(scores, labels_batch)\n",
    "        \n",
    "        accuracy = check_accuracy_batch(scores, labels_batch)\n",
    "        loss_history.append(loss)\n",
    "        test_accuracy_history.append(accuracy)\n",
    "    \n",
    "    \n",
    "    return loss_history, test_accuracy_history\n",
    "\n",
    "    #if dataloader.train == True:\n",
    "    #    plt.ylabel('Validation accuracy')\n",
    "    #else:\n",
    "    #    plt.ylabel('Test accuracy')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"model = model.to(device=device) # move model parameters to CPU/GPU\n",
    "    for t, (x, y) in enumerate(loader_train):\n",
    "        model.train()\n",
    "        x = x.to(device=device, dtype= dtype)\n",
    "        y = y.to(device=device, dtype=torch.long)\n",
    "        scores = model(x)\n",
    "        loss = F.cross_entropy(scores, y)\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "        if t % 100 == 0:\n",
    "            print('Iteration %d, loss = %4f' % (t, loss.item()))\n",
    "            check_accuracy(loader_val, model)\n",
    "            print()\n",
    "        if t == 32000 or t == 48000:\n",
    "            lr /= 10\n",
    "        if t == 64000:\n",
    "            return\n",
    "                \n",
    "def check_accuracy(loader, model):\n",
    "    if loader.dataset.train:\n",
    "        print('Checking accuracy on validation set')\n",
    "    else: \n",
    "        print('Checking accuracy on test set')\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval() # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=dtype) # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (y == preds).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kathrinparchatka/Development/ML/ML-ENV/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 13 / 128 correct (10.16)\n",
      "Got 27 / 128 correct (21.09)\n",
      "Got 29 / 128 correct (22.66)\n",
      "Got 24 / 128 correct (18.75)\n",
      "Got 26 / 128 correct (20.31)\n",
      "Got 38 / 128 correct (29.69)\n",
      "Got 42 / 128 correct (32.81)\n",
      "Got 40 / 128 correct (31.25)\n",
      "Got 40 / 128 correct (31.25)\n",
      "Got 50 / 128 correct (39.06)\n",
      "Got 40 / 128 correct (31.25)\n",
      "Got 42 / 128 correct (32.81)\n",
      "Got 40 / 128 correct (31.25)\n",
      "Got 56 / 128 correct (43.75)\n",
      "Got 44 / 128 correct (34.38)\n",
      "Got 55 / 128 correct (42.97)\n",
      "Got 50 / 128 correct (39.06)\n",
      "Got 52 / 128 correct (40.62)\n",
      "Got 57 / 128 correct (44.53)\n",
      "Got 43 / 128 correct (33.59)\n",
      "Got 45 / 128 correct (35.16)\n",
      "Got 52 / 128 correct (40.62)\n",
      "Got 77 / 128 correct (60.16)\n",
      "Got 59 / 128 correct (46.09)\n",
      "Got 63 / 128 correct (49.22)\n",
      "Got 55 / 128 correct (42.97)\n",
      "Got 69 / 128 correct (53.91)\n",
      "Got 61 / 128 correct (47.66)\n",
      "Got 56 / 128 correct (43.75)\n",
      "Got 69 / 128 correct (53.91)\n",
      "Got 61 / 128 correct (47.66)\n",
      "Got 61 / 128 correct (47.66)\n",
      "Got 60 / 128 correct (46.88)\n",
      "Got 68 / 128 correct (53.12)\n",
      "Got 68 / 128 correct (53.12)\n",
      "Got 72 / 128 correct (56.25)\n",
      "Epoch  1\n",
      "Got 61 / 128 correct (47.66)\n",
      "Got 76 / 128 correct (59.38)\n",
      "Got 57 / 128 correct (44.53)\n",
      "Got 67 / 128 correct (52.34)\n",
      "Got 81 / 128 correct (63.28)\n",
      "Got 81 / 128 correct (63.28)\n",
      "Got 77 / 128 correct (60.16)\n",
      "Got 66 / 128 correct (51.56)\n",
      "Got 79 / 128 correct (61.72)\n",
      "Got 71 / 128 correct (55.47)\n",
      "Got 76 / 128 correct (59.38)\n",
      "Got 78 / 128 correct (60.94)\n",
      "Got 65 / 128 correct (50.78)\n",
      "Got 73 / 128 correct (57.03)\n",
      "Got 77 / 128 correct (60.16)\n",
      "Got 70 / 128 correct (54.69)\n",
      "Got 70 / 128 correct (54.69)\n",
      "Got 84 / 128 correct (65.62)\n",
      "Got 79 / 128 correct (61.72)\n",
      "Got 83 / 128 correct (64.84)\n",
      "Got 75 / 128 correct (58.59)\n",
      "Got 89 / 128 correct (69.53)\n",
      "Got 77 / 128 correct (60.16)\n",
      "Got 82 / 128 correct (64.06)\n",
      "Got 86 / 128 correct (67.19)\n",
      "Got 80 / 128 correct (62.50)\n",
      "Got 76 / 128 correct (59.38)\n",
      "Got 70 / 128 correct (54.69)\n",
      "Got 88 / 128 correct (68.75)\n",
      "Got 84 / 128 correct (65.62)\n",
      "Got 86 / 128 correct (67.19)\n",
      "Got 75 / 128 correct (58.59)\n",
      "Got 86 / 128 correct (67.19)\n",
      "Got 82 / 128 correct (64.06)\n",
      "Got 81 / 128 correct (63.28)\n",
      "Got 87 / 128 correct (67.97)\n",
      "Epoch  2\n",
      "Got 81 / 128 correct (63.28)\n",
      "Got 80 / 128 correct (62.50)\n",
      "Got 81 / 128 correct (63.28)\n",
      "Got 89 / 128 correct (69.53)\n",
      "Got 97 / 128 correct (75.78)\n",
      "Got 86 / 128 correct (67.19)\n",
      "Got 81 / 128 correct (63.28)\n",
      "Got 98 / 128 correct (76.56)\n",
      "Got 81 / 128 correct (63.28)\n",
      "Got 93 / 128 correct (72.66)\n",
      "Got 84 / 128 correct (65.62)\n",
      "Got 82 / 128 correct (64.06)\n",
      "Got 87 / 128 correct (67.97)\n",
      "Got 98 / 128 correct (76.56)\n",
      "Got 78 / 128 correct (60.94)\n",
      "Got 87 / 128 correct (67.97)\n",
      "Got 83 / 128 correct (64.84)\n",
      "Got 87 / 128 correct (67.97)\n",
      "Got 93 / 128 correct (72.66)\n",
      "Got 90 / 128 correct (70.31)\n",
      "Got 100 / 128 correct (78.12)\n",
      "Got 83 / 128 correct (64.84)\n",
      "Got 93 / 128 correct (72.66)\n",
      "Got 86 / 128 correct (67.19)\n",
      "Got 89 / 128 correct (69.53)\n",
      "Got 93 / 128 correct (72.66)\n",
      "Got 93 / 128 correct (72.66)\n",
      "Got 76 / 128 correct (59.38)\n",
      "Got 90 / 128 correct (70.31)\n",
      "Got 90 / 128 correct (70.31)\n",
      "Got 87 / 128 correct (67.97)\n",
      "Got 87 / 128 correct (67.97)\n",
      "Got 90 / 128 correct (70.31)\n",
      "Got 87 / 128 correct (67.97)\n",
      "Got 89 / 128 correct (69.53)\n",
      "Got 92 / 128 correct (71.88)\n"
     ]
    }
   ],
   "source": [
    "# Check implementation\n",
    "n = 3\n",
    "lr = 0.1\n",
    "model = ResNet(n)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=0.0001)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, 0.95)\n",
    "overall_train_loss, overall_val_loss, overall_train_acc, overall_val_acc = [], [], [], []\n",
    "\n",
    "for epoch in range(3):\n",
    "    print('Epoch ', epoch)\n",
    "    scheduler.step()\n",
    "    train_loss, train_acc = train_ResNet(model, optimizer, loader_train)\n",
    "    val_loss, val_acc = evaluate_ResNet(model, loader_val)\n",
    "    overall_train_loss += train_loss\n",
    "    overall_train_acc += train_acc\n",
    "    overall_val_loss += val_loss\n",
    "    overall_val_acc += val_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'overall_train_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2a69ec3f5ffe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Training'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Validation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mplot_loss_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moverall_train_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverall_train_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverall_val_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverall_val_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moverall_train_loss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moverall_val_loss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'overall_train_loss' is not defined"
     ]
    }
   ],
   "source": [
    "# plot loss and accuracy curves\n",
    "def plot_loss_acc(train_loss, train_acc, val_loss, val_acc):   \n",
    "    print('Loss')\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(train_loss)\n",
    "    plt.plot(val_loss)\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(['Training', 'Validation'])\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(train_acc)\n",
    "    plt.plot(val_acc)\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend(['Training', 'Validation'])\n",
    "    \n",
    "plot_loss_acc(overall_train_loss, overall_train_acc, overall_val_loss, overall_val_acc)\n",
    "print(overall_train_loss[0])\n",
    "print(overall_val_loss[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[tensor(4.0431, grad_fn=<NllLossBackward>),\n",
       "  tensor(3.2244, grad_fn=<NllLossBackward>),\n",
       "  tensor(3.3097, grad_fn=<NllLossBackward>),\n",
       "  tensor(3.1141, grad_fn=<NllLossBackward>),\n",
       "  tensor(3.4346, grad_fn=<NllLossBackward>),\n",
       "  tensor(2.6042, grad_fn=<NllLossBackward>),\n",
       "  tensor(2.4679, grad_fn=<NllLossBackward>),\n",
       "  tensor(2.6407, grad_fn=<NllLossBackward>),\n",
       "  tensor(2.2931, grad_fn=<NllLossBackward>),\n",
       "  tensor(2.7352, grad_fn=<NllLossBackward>),\n",
       "  tensor(2.3425, grad_fn=<NllLossBackward>),\n",
       "  tensor(2.3787, grad_fn=<NllLossBackward>),\n",
       "  tensor(2.0823, grad_fn=<NllLossBackward>),\n",
       "  tensor(2.7473, grad_fn=<NllLossBackward>),\n",
       "  tensor(2.4665, grad_fn=<NllLossBackward>),\n",
       "  tensor(2.5150, grad_fn=<NllLossBackward>),\n",
       "  tensor(2.1866, grad_fn=<NllLossBackward>),\n",
       "  tensor(2.1648, grad_fn=<NllLossBackward>),\n",
       "  tensor(2.0765, grad_fn=<NllLossBackward>),\n",
       "  tensor(2.1105, grad_fn=<NllLossBackward>),\n",
       "  tensor(2.1374, grad_fn=<NllLossBackward>),\n",
       "  tensor(2.0693, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.8742, grad_fn=<NllLossBackward>),\n",
       "  tensor(2.1393, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.8413, grad_fn=<NllLossBackward>),\n",
       "  tensor(2.0473, grad_fn=<NllLossBackward>),\n",
       "  tensor(2.1676, grad_fn=<NllLossBackward>),\n",
       "  tensor(2.0354, grad_fn=<NllLossBackward>),\n",
       "  tensor(2.2327, grad_fn=<NllLossBackward>),\n",
       "  tensor(2.2261, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.9523, grad_fn=<NllLossBackward>),\n",
       "  tensor(2.1026, grad_fn=<NllLossBackward>),\n",
       "  tensor(2.0899, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.8985, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.9053, grad_fn=<NllLossBackward>),\n",
       "  tensor(2.4908, grad_fn=<NllLossBackward>)],\n",
       " [tensor(2.0918, grad_fn=<NllLossBackward>),\n",
       "  tensor(2.1766, grad_fn=<NllLossBackward>),\n",
       "  tensor(2.3525, grad_fn=<NllLossBackward>),\n",
       "  tensor(2.4703, grad_fn=<NllLossBackward>),\n",
       "  tensor(2.1242, grad_fn=<NllLossBackward>),\n",
       "  tensor(2.0434, grad_fn=<NllLossBackward>),\n",
       "  tensor(2.3222, grad_fn=<NllLossBackward>),\n",
       "  tensor(2.0569, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.9288, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.8912, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.9159, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.7738, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.9462, grad_fn=<NllLossBackward>),\n",
       "  tensor(2.0084, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.8084, grad_fn=<NllLossBackward>),\n",
       "  tensor(2.0219, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.7219, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.8038, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.8318, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.8605, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.7797, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.8615, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.9523, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.7582, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.8817, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.9293, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.7677, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.7824, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.6508, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.8608, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.8402, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.8006, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.7565, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.8326, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.8343, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.5517, grad_fn=<NllLossBackward>)],\n",
       " [tensor(1.8039, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.8439, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.6521, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.8551, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.8302, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.7501, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.7374, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.8914, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.7933, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.7598, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.7897, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.7075, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.7615, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.7239, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.7526, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.8302, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.7247, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.8246, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.6700, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.7138, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.6906, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.6815, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.6533, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.8280, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.7218, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.6653, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.6999, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.8388, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.7611, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.6314, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.6849, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.7230, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.6831, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.8216, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.7625, grad_fn=<NllLossBackward>),\n",
       "  tensor(1.9062, grad_fn=<NllLossBackward>)]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
